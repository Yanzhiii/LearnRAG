{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanzhiii/LearnRAG/blob/main/LearnRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZecW6EIzMuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbfee95-988e-45aa-8cff-9dffd7b66d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q rank_bm25\n",
        "!pip install -q chromadb\n",
        "!pip install -qU bitsandbytes accelerate\n",
        "!pip install -q langchain_community\n",
        "!pip install -qU peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsuuAvdHCPR5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import json\n",
        "import os\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from google.colab import userdata\n",
        "from peft import PeftModel\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_typITj8Mj3",
        "outputId": "62b0e88d-7135-4422-b019-cd7aafdb9cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Hugging Face token in Colab Secret\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ],
      "metadata": {
        "id": "I_gTmIy9_Ma1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improved_rag = False\n",
        "debug_mode = True    # To skip certain cells"
      ],
      "metadata": {
        "id": "GWJdrfKSloxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's the meaning of embeddings in NLP?\""
      ],
      "metadata": {
        "id": "mdu_PsQQQWbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbyCLoTdN4cu"
      },
      "source": [
        "## Data Collector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Data Collector"
      ],
      "metadata": {
        "id": "6EcxjaCRjOzB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-MdofN0N9wD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acd4f75-4d6b-4201-dda7-efa74859563d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ],
      "source": [
        "if improved_rag:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "\n",
        "  def scrape_p(url):\n",
        "      response = requests.get(url)\n",
        "      soup = BeautifulSoup(response.content,\"html.parser\")\n",
        "      paragraphs = soup.find_all(\"p\")\n",
        "      scraped_doc = []\n",
        "      for p in paragraphs:\n",
        "          scraped_doc.append(p.get_text())\n",
        "\n",
        "      return scraped_doc\n",
        "\n",
        "  doc_1 = \"https://www.ibm.com/think/topics/machine-learning\"\n",
        "  doc_2 = \"https://www.ibm.com/think/topics/support-vector-machine\"\n",
        "  database = list(set(scrape_p(doc_1) + scrape_p(doc_2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRlBIvMeeIe"
      },
      "source": [
        "## Retriver"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Retriver using BM25\n",
        "BM25 repo: https://github.com/dorianbrown/rank_bm25"
      ],
      "metadata": {
        "id": "LXGsrM4xjBmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrx3UiKmB9fl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc2f697-72fd-4839-9777-6a95a09669dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ],
      "source": [
        "if improved_rag:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "\n",
        "  top_n = 6\n",
        "  punc = string.punctuation\n",
        "  clean_tokenized_docs = []\n",
        "\n",
        "  # split by \" \" then remove punctuation\n",
        "  tokenized_docs = [doc.split(\" \") for doc in database]\n",
        "  for i in range(len(tokenized_docs)):\n",
        "      clean_tokenized_docs += [[word.strip(punc) for word in tokenized_docs[i]]]\n",
        "\n",
        "  bm25 = BM25Okapi(clean_tokenized_docs)\n",
        "  clean_tokenized_query = [word.strip(punc) for word in query.split(\" \")]\n",
        "  doc_scores = bm25.get_scores(clean_tokenized_query)   # get the indeces of docs from small to big\n",
        "  top_n_indices = np.argsort(doc_scores)[-top_n:][::-1] # get the top-n docs\n",
        "  top_n_docs = [database[i] for i in top_n_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved Retriever using Chroma DB"
      ],
      "metadata": {
        "id": "rW_ovLrWGqbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Loads the chromadb collection from text files\n",
        "def load_collection(collection,docs_directory,chunk_size=100,chunk_overlap=30):\n",
        "    raw_docs = []\n",
        "    for (root, dirs, file) in os.walk(docs_directory):\n",
        "        raw_docs = file\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    length_function=len,\n",
        "    add_start_index=True,\n",
        "    )\n",
        "\n",
        "    for raw_doc in raw_docs:\n",
        "        id = ''\n",
        "        doc = ''\n",
        "        with open('/'.join([docs_directory,raw_doc]),\"r\",encoding=\"utf-8\") as file:\n",
        "            id = raw_doc.removesuffix('.txt').replace('_','/')\n",
        "            doc = file.readline()\n",
        "            docs = text_splitter.split_text(doc)\n",
        "            docs_len = len(docs)\n",
        "            ids = []\n",
        "            for i in range(docs_len):\n",
        "              ids.append(id+'_'+str(i))\n",
        "        collection.add(documents=docs,ids=ids)"
      ],
      "metadata": {
        "id": "bwM_20k1GqI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Latest progress 25-03-2025_"
      ],
      "metadata": {
        "id": "RFjBrvhF0Zlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Loads chromadb collection from pdfs\n",
        "def load_collection_pdf(collection,docs_directory,chunk_size=300,chunk_overlap=100):\n",
        "    raw_docs = []\n",
        "    for (root, dirs, file) in os.walk(docs_directory):\n",
        "        raw_docs = file\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    length_function=len,\n",
        "    add_start_index=True,\n",
        ")\n",
        "\n",
        "    for raw_doc in raw_docs[:10]:\n",
        "        id = ''\n",
        "        doc = ''\n",
        "        loader = PyPDFLoader('/'.join([docs_directory,raw_doc]))\n",
        "        document = loader.load()\n",
        "        for docpart in document:\n",
        "          doc = ' '.join([doc,docpart.page_content])\n",
        "        docs = text_splitter.split_text(doc)\n",
        "        docs_len = len(docs)\n",
        "        ids = []\n",
        "        for i in range(docs_len):\n",
        "          ids.append(id+'_'+str(i))\n",
        "        collection.add(documents=docs,ids=ids)"
      ],
      "metadata": {
        "id": "D-kwAhb4o8WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/chroma.zip'"
      ],
      "metadata": {
        "id": "kxrevzeK0iIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3666a040-b3cf-4157-ffa1-72fc7750eb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/chroma.zip\n",
            "   creating: chroma/\n",
            "  inflating: __MACOSX/._chroma       \n",
            "   creating: chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/\n",
            "  inflating: __MACOSX/chroma/._ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e  \n",
            "  inflating: chroma/chroma.sqlite3   \n",
            "  inflating: __MACOSX/chroma/._chroma.sqlite3  \n",
            "  inflating: chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/data_level0.bin  \n",
            "  inflating: __MACOSX/chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/._data_level0.bin  \n",
            "  inflating: chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/length.bin  \n",
            "  inflating: __MACOSX/chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/._length.bin  \n",
            "  inflating: chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/link_lists.bin  \n",
            "  inflating: __MACOSX/chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/._link_lists.bin  \n",
            "  inflating: chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/header.bin  \n",
            "  inflating: __MACOSX/chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/._header.bin  \n",
            "  inflating: chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/index_metadata.pickle  \n",
            "  inflating: __MACOSX/chroma/ae6b55bb-c4c4-42b0-b261-c5699a2d1f7e/._index_metadata.pickle  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "chroma_client = chromadb.PersistentClient(path='/content/chroma')"
      ],
      "metadata": {
        "id": "m2Ff2Jzg0oZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chroma_client.list_collections())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB8UHWn77SDR",
        "outputId": "ba75b85b-7a94-4e57-9fc1-ecbfe295a631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Collection(name=all_collection)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ml_sample = chroma_client.get_collection(name=\"all_collection\")"
      ],
      "metadata": {
        "id": "1g4vb3620sWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriver_output = ml_sample.query(\n",
        "    query_texts = [query],\n",
        "    n_results = 5\n",
        ")\n",
        "retriver_output = [chunk.replace('\\n', ' ') for chunk in retriver_output['documents'][0]]"
      ],
      "metadata": {
        "id": "T7UGjb_h0uqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca54ee3-e20e-4798-cb04-95c0e5c86897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 36.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriver_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Wubcoqo5yW",
        "outputId": "6fe5938a-ae00-43ce-cab2-8bfc87e4a66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Embeddings are long vectors of real numbers and provide a way to represent  the knowledge associated with the tokens. During training, a model implicitly  deﬁnes a representation space that determines the meaning of embeddings. Usually,  embeddings are assigned to tokens, i.e. parts of words, but may also be determined  for paragraphs and complete documents. If two embeddings have a small vector  distance, the meaning of the underlying tokens is similar. Foundation Models  generate increasingly reﬁned embeddings in their layers by taking into account the  context of the tokens. The word “bank” close to the word “money” has a different  embedding than a “bank” close to the word “river”, making the embeddings  contextual. These effects also apply to tokens of different media types.  Embeddings are calculated by self-attention computing correlations between  linear projections of input embeddings. This is done in parallel by multiple linear',\n",
              " 'Internet to train the models. In the second step, the model was ﬁne-tuned with  a few-thousand manually annotated sentences to solve a speciﬁc task, such as  determining, whether a movie review expresses a positive sentiment. The approach  worked extremely well, showing that the models have the capability to detect  subtle semantic properties of language. This two-step procedure was called transfer  learning. After extensive experimentation, it was found that these models worked  better the bigger they became and the more data their training sets contained.  Knowledge in PLMs is stored by a huge number of parameters. Parameters  contain the recipe to compute embeddings for the input tokens of the models. 8.1 Foundation Models Are a New Paradigm 385 Embeddings are long vectors of real numbers and provide a way to represent  the knowledge associated with the tokens. During training, a model implicitly  deﬁnes a representation space that determines the meaning of embeddings. Usually,',\n",
              " 'that StyleLM has problems with content reproduction. They propose to pre-train  the encoder-decoder .DECw(ENCu(x)) on a large generic corpus. Afterwards the  encoder-decoder is ﬁne-tuned on the text of the target author.  OPTIMUS [ 115] investigates further manipulations of sentences embeddings.  An encoder with parameter . u is required to generate a latent vector from text . z= ENCu(x). It is initialized with a pre-trained BERT model. A linearly transformed  version .z=W∗h [CLS] of the embedding of the ﬁrst token [CLS] of a sentence is  deﬁned as latent representation. The generator (decoder) with parameter . w generates  the text sequence .x=D ECw(z) from a random vector . z (e.g. multivariate Gaussian)  with prior .p(z). The authors start with a pre-trained GPT-2 model as decoder. . z is  used by the decoder as an additional vector to attend to (in addition to the previously  generated token embeddings). Both networks .˜x=D ECw(ENCu(x)) are trained',\n",
              " 'by the same PLM (BERT or RoBERTa). Similarity was compared by the cosine  similarity  .φ(ηq(q),η d(di))= ηq(q)⊺ηd(di)\\ued79\\ued79 ηq(q) \\ued79\\ued79∗ \\ued79\\ued79ηd(di) \\ued79\\ued79. (6.6)  To generate sentence embeddings the authors investigated three alternatives. (1)  Use the embedding of the [CLS] token. (2) Averaging (mean-pooling) of all  output embeddings. (3) Component-wise maximum (max-pooling) of all output  embeddings. Without ﬁne-tuning the results were worse than for non-contextual  embeddings. Fine-tuning boosted performance and yields a new S OTA. It turned out  that average pooling was the most effective design, slightly better than max pooling  or using the [CLS] token. Most important the computation time for ﬁnding the best  match in 10,000 documents was reduced from 65 h to 5 s.  DPR [ 94] used separate encoders .ηq(q) and .ηd(di) for the query q and the text  passages . di of about 100 words. Both encoders took the [CLS] embedding from  BERT.BASE as its output representation. As comparison function the inner product',\n",
              " 'embeddings are concatenated and used as input to a logistic classiﬁer to estimate  the probability of the possible relations (or ‘no relation’). Pre-trained variants of  BERT are ﬁne-tuned with ACE 2005 to predict the relations. With a BERT. BASE model of 105M parameters the approach yields an F1-value of 68.8% on the ACE05  benchmark. If ALBERT.XXLARGE [45] with 235M parameters is used to compute the  embeddings, the F1-score grows to 72.3%.  For NER, the PL-Marker model uses a similar approach. For each possible  span in the input starting at token . vi and ending at token .vj,j≥i , leviated markers  are created, which do not affect the embeddings of the normal tokens. Again the  embeddings of the start and end tokens of a span as well as the embeddings of  leviated markers are input for a logistic classiﬁer computing the probability of the 5.4 Relation Extraction 213 “This Must Be the Place” is a song by new wave band  Talking Heads, released in November 1983 as the']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7_DIeBOtH0F"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fiGOsLtFR6"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = \"\\n\".join(retriver_output)\n",
        "rag_prompts = \"You are a learning assistance chatbot. Based only on the provided knowledge base, \\\n",
        "answer the user's question. If the knowledge base does not contain relevant information \\\n",
        "to answer the question, clearly state that you do not know and cannot answer \\\n",
        "based on the available information. When you can answer, explain clearly, \\\n",
        "providing relevant details and background information found within the text. \\\n",
        "Do not use any prior knowledge. \\n\\\n",
        "Knowledge base: \\n{}\\n\\\n",
        "Question: {}\\n\\\n",
        "Indicate what you refer to in the knowledge database briefly, \\\n",
        "Answer the question concisely with nice structure and fluent logic in one paragraph:\\n\".format(retrieved_docs, query)\n",
        "\n",
        "plain_prompts = \"You are a learning assistance chatbot answering the given question.\\n\\\n",
        "Question: {}\\n\\\n",
        "Following the instruction at beginning, answer the question in your words concisely with in one paragraph:\\n\\n\".format(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For fine-tuned RAFT model (Structured Messages Format)\n",
        "# Format documents with tags to match training format\n",
        "formatted_docs = \"\"\n",
        "for doc in retriver_output:\n",
        "  formatted_docs += f\"<DOCUMENT>{doc}</DOCUMENT>\\n\"\n",
        "\n",
        "system_prompt = \"You are a learning assistance chatbot. Based only on the provided knowledge base, answer the user's question.\\\n",
        " If the knowledge base does not contain relevant information to answer the question, clearly state that you do not know and cannot \\\n",
        " answer based on the available information. When you can answer, explain clearly, providing relevant details and background \\\n",
        " information found within the text. Do not use any prior knowledge.\"\n",
        "user_prompt = f\"{formatted_docs}\\nQuestion: {query}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "]"
      ],
      "metadata": {
        "id": "zrUYAOKCSdS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s595gedXcC3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65517a33-6f6d-43cd-f051-e1af3e8e64ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a learning assistance chatbot. Based only on the provided knowledge base, answer the user's question. If the knowledge base does not contain relevant information to answer the question, clearly state that you do not know and cannot answer based on the available information. When you can answer, explain clearly, providing relevant details and background information found within the text. Do not use any prior knowledge. \n",
            "Knowledge base: \n",
            "Embeddings are long vectors of real numbers and provide a way to represent  the knowledge associated with the tokens. During training, a model implicitly  deﬁnes a representation space that determines the meaning of embeddings. Usually,  embeddings are assigned to tokens, i.e. parts of words, but may also be determined  for paragraphs and complete documents. If two embeddings have a small vector  distance, the meaning of the underlying tokens is similar. Foundation Models  generate increasingly reﬁned embeddings in their layers by taking into account the  context of the tokens. The word “bank” close to the word “money” has a different  embedding than a “bank” close to the word “river”, making the embeddings  contextual. These effects also apply to tokens of different media types.  Embeddings are calculated by self-attention computing correlations between  linear projections of input embeddings. This is done in parallel by multiple linear\n",
            "Internet to train the models. In the second step, the model was ﬁne-tuned with  a few-thousand manually annotated sentences to solve a speciﬁc task, such as  determining, whether a movie review expresses a positive sentiment. The approach  worked extremely well, showing that the models have the capability to detect  subtle semantic properties of language. This two-step procedure was called transfer  learning. After extensive experimentation, it was found that these models worked  better the bigger they became and the more data their training sets contained.  Knowledge in PLMs is stored by a huge number of parameters. Parameters  contain the recipe to compute embeddings for the input tokens of the models. 8.1 Foundation Models Are a New Paradigm 385 Embeddings are long vectors of real numbers and provide a way to represent  the knowledge associated with the tokens. During training, a model implicitly  deﬁnes a representation space that determines the meaning of embeddings. Usually,\n",
            "that StyleLM has problems with content reproduction. They propose to pre-train  the encoder-decoder .DECw(ENCu(x)) on a large generic corpus. Afterwards the  encoder-decoder is ﬁne-tuned on the text of the target author.  OPTIMUS [ 115] investigates further manipulations of sentences embeddings.  An encoder with parameter . u is required to generate a latent vector from text . z= ENCu(x). It is initialized with a pre-trained BERT model. A linearly transformed  version .z=W∗h [CLS] of the embedding of the ﬁrst token [CLS] of a sentence is  deﬁned as latent representation. The generator (decoder) with parameter . w generates  the text sequence .x=D ECw(z) from a random vector . z (e.g. multivariate Gaussian)  with prior .p(z). The authors start with a pre-trained GPT-2 model as decoder. . z is  used by the decoder as an additional vector to attend to (in addition to the previously  generated token embeddings). Both networks .˜x=D ECw(ENCu(x)) are trained\n",
            "by the same PLM (BERT or RoBERTa). Similarity was compared by the cosine  similarity  .φ(ηq(q),η d(di))= ηq(q)⊺ηd(di) ηq(q) ∗ ηd(di) . (6.6)  To generate sentence embeddings the authors investigated three alternatives. (1)  Use the embedding of the [CLS] token. (2) Averaging (mean-pooling) of all  output embeddings. (3) Component-wise maximum (max-pooling) of all output  embeddings. Without ﬁne-tuning the results were worse than for non-contextual  embeddings. Fine-tuning boosted performance and yields a new S OTA. It turned out  that average pooling was the most effective design, slightly better than max pooling  or using the [CLS] token. Most important the computation time for ﬁnding the best  match in 10,000 documents was reduced from 65 h to 5 s.  DPR [ 94] used separate encoders .ηq(q) and .ηd(di) for the query q and the text  passages . di of about 100 words. Both encoders took the [CLS] embedding from  BERT.BASE as its output representation. As comparison function the inner product\n",
            "embeddings are concatenated and used as input to a logistic classiﬁer to estimate  the probability of the possible relations (or ‘no relation’). Pre-trained variants of  BERT are ﬁne-tuned with ACE 2005 to predict the relations. With a BERT. BASE model of 105M parameters the approach yields an F1-value of 68.8% on the ACE05  benchmark. If ALBERT.XXLARGE [45] with 235M parameters is used to compute the  embeddings, the F1-score grows to 72.3%.  For NER, the PL-Marker model uses a similar approach. For each possible  span in the input starting at token . vi and ending at token .vj,j≥i , leviated markers  are created, which do not affect the embeddings of the normal tokens. Again the  embeddings of the start and end tokens of a span as well as the embeddings of  leviated markers are input for a logistic classiﬁer computing the probability of the 5.4 Relation Extraction 213 “This Must Be the Place” is a song by new wave band  Talking Heads, released in November 1983 as the\n",
            "Question: What's the meaning of embeddings in NLP?\n",
            "Indicate what you refer to in the knowledge database briefly, Answer the question concisely with nice structure and fluent logic in one paragraph:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(rag_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP85sEldhIIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062d2e8b-beed-4a28-c4f4-3048cdf294db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a learning assistance chatbot answering the given question.\n",
            "Question: What's the meaning of embeddings in NLP?\n",
            "Following the instruction at beginning, answer the question in your words concisely with in one paragraph:\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(plain_prompts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPn3QPV1SjIi",
        "outputId": "6caef7e4-f90a-4108-8baf-ca9fb448f4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are a learning assistance chatbot. Based only on the provided knowledge base, answer the user's question. If the knowledge base does not contain relevant information to answer the question, clearly state that you do not know and cannot answer based on the available information. When you can answer, explain clearly, providing relevant details and background information found within the text. Do not use any prior knowledge.\"},\n",
              " {'role': 'user',\n",
              "  'content': \"<DOCUMENT>Embeddings are long vectors of real numbers and provide a way to represent  the knowledge associated with the tokens. During training, a model implicitly  deﬁnes a representation space that determines the meaning of embeddings. Usually,  embeddings are assigned to tokens, i.e. parts of words, but may also be determined  for paragraphs and complete documents. If two embeddings have a small vector  distance, the meaning of the underlying tokens is similar. Foundation Models  generate increasingly reﬁned embeddings in their layers by taking into account the  context of the tokens. The word “bank” close to the word “money” has a different  embedding than a “bank” close to the word “river”, making the embeddings  contextual. These effects also apply to tokens of different media types.  Embeddings are calculated by self-attention computing correlations between  linear projections of input embeddings. This is done in parallel by multiple linear</DOCUMENT>\\n<DOCUMENT>Internet to train the models. In the second step, the model was ﬁne-tuned with  a few-thousand manually annotated sentences to solve a speciﬁc task, such as  determining, whether a movie review expresses a positive sentiment. The approach  worked extremely well, showing that the models have the capability to detect  subtle semantic properties of language. This two-step procedure was called transfer  learning. After extensive experimentation, it was found that these models worked  better the bigger they became and the more data their training sets contained.  Knowledge in PLMs is stored by a huge number of parameters. Parameters  contain the recipe to compute embeddings for the input tokens of the models. 8.1 Foundation Models Are a New Paradigm 385 Embeddings are long vectors of real numbers and provide a way to represent  the knowledge associated with the tokens. During training, a model implicitly  deﬁnes a representation space that determines the meaning of embeddings. Usually,</DOCUMENT>\\n<DOCUMENT>that StyleLM has problems with content reproduction. They propose to pre-train  the encoder-decoder .DECw(ENCu(x)) on a large generic corpus. Afterwards the  encoder-decoder is ﬁne-tuned on the text of the target author.  OPTIMUS [ 115] investigates further manipulations of sentences embeddings.  An encoder with parameter . u is required to generate a latent vector from text . z= ENCu(x). It is initialized with a pre-trained BERT model. A linearly transformed  version .z=W∗h [CLS] of the embedding of the ﬁrst token [CLS] of a sentence is  deﬁned as latent representation. The generator (decoder) with parameter . w generates  the text sequence .x=D ECw(z) from a random vector . z (e.g. multivariate Gaussian)  with prior .p(z). The authors start with a pre-trained GPT-2 model as decoder. . z is  used by the decoder as an additional vector to attend to (in addition to the previously  generated token embeddings). Both networks .˜x=D ECw(ENCu(x)) are trained</DOCUMENT>\\n<DOCUMENT>by the same PLM (BERT or RoBERTa). Similarity was compared by the cosine  similarity  .φ(ηq(q),η d(di))= ηq(q)⊺ηd(di)\\ued79\\ued79 ηq(q) \\ued79\\ued79∗ \\ued79\\ued79ηd(di) \\ued79\\ued79. (6.6)  To generate sentence embeddings the authors investigated three alternatives. (1)  Use the embedding of the [CLS] token. (2) Averaging (mean-pooling) of all  output embeddings. (3) Component-wise maximum (max-pooling) of all output  embeddings. Without ﬁne-tuning the results were worse than for non-contextual  embeddings. Fine-tuning boosted performance and yields a new S OTA. It turned out  that average pooling was the most effective design, slightly better than max pooling  or using the [CLS] token. Most important the computation time for ﬁnding the best  match in 10,000 documents was reduced from 65 h to 5 s.  DPR [ 94] used separate encoders .ηq(q) and .ηd(di) for the query q and the text  passages . di of about 100 words. Both encoders took the [CLS] embedding from  BERT.BASE as its output representation. As comparison function the inner product</DOCUMENT>\\n<DOCUMENT>embeddings are concatenated and used as input to a logistic classiﬁer to estimate  the probability of the possible relations (or ‘no relation’). Pre-trained variants of  BERT are ﬁne-tuned with ACE 2005 to predict the relations. With a BERT. BASE model of 105M parameters the approach yields an F1-value of 68.8% on the ACE05  benchmark. If ALBERT.XXLARGE [45] with 235M parameters is used to compute the  embeddings, the F1-score grows to 72.3%.  For NER, the PL-Marker model uses a similar approach. For each possible  span in the input starting at token . vi and ending at token .vj,j≥i , leviated markers  are created, which do not affect the embeddings of the normal tokens. Again the  embeddings of the start and end tokens of a span as well as the embeddings of  leviated markers are input for a logistic classiﬁer computing the probability of the 5.4 Relation Extraction 213 “This Must Be the Place” is a song by new wave band  Talking Heads, released in November 1983 as the</DOCUMENT>\\n\\nQuestion: What's the meaning of embeddings in NLP?\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts Improvements"
      ],
      "metadata": {
        "id": "v3NynYyZIFJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Expansion using Hugging Face Transformers"
      ],
      "metadata": {
        "id": "_c2ssukRrTJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_mode:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "  # HuggingFace Transformers\n",
        "  from transformers import pipeline\n",
        "\n",
        "  reformulate = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
        "  query = \"classification?\"\n",
        "  expanded = reformulate(f\"Expand this question for learning assistance: {query}\")\n",
        "  print(expanded[0]['generated_text'])"
      ],
      "metadata": {
        "id": "4mkUF_g9HuKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc97d55-9d75-45a0-e7c9-03758d457e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Thresholding Before Generation"
      ],
      "metadata": {
        "id": "LEPP6KyNrxos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_mode:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "  from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "  model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "  doc_embeddings = model.encode(retrieved_docs, convert_to_tensor=True)\n",
        "\n",
        "  similarities = util.pytorch_cos_sim(query_embedding, doc_embeddings)\n",
        "  max_sim = similarities.max().item()\n",
        "\n",
        "  if max_sim < 0.4:\n",
        "      print(\"User query is vague. Ask for more information.\")\n",
        "  else:\n",
        "      print(\"Proceed with generation.\")"
      ],
      "metadata": {
        "id": "3vVQqWmzrCZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f86a41b-f7c7-4d82-d01d-f62a3c3b26ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few Shot"
      ],
      "metadata": {
        "id": "qscluEezsP4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_mode:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "  system_prompts = \"\"\"\n",
        "  You are a helpful learning chatbot. Use the examples below to guide your behavior.\n",
        "\n",
        "  Example 1:\n",
        "  User: types\n",
        "  Assistant: Could you clarify what you mean by 'types'? Are you referring to data types or something else?\n",
        "\n",
        "  Example 2:\n",
        "  User: What are the types of supervised learning?\n",
        "  Assistant: Supervised learning includes classification and regression.\n",
        "\n",
        "  User: {question}\n",
        "  Assistant:\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "iBhXmxRHr8Hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0611bf-1d00-4e34-b5a6-0ae98baad26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoT"
      ],
      "metadata": {
        "id": "PLXZV2io5SEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_mode:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "  def build_cot_prompt(question, context):\n",
        "      return f\"\"\"\n",
        "  You are a learning assistant helping users understand complex concepts.\n",
        "\n",
        "  Question: {question}\n",
        "  Context: {context}\n",
        "\n",
        "  Let's think step-by-step to answer this question:\n",
        "  1.\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "lODQZIhp5sTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744cd5e6-7b44-49f0-9da0-0357620dc2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_mode:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "  def generate_with_cot(question, retrieved_docs, tokenizer, model, device):\n",
        "      prompt = build_cot_prompt(question, '\\n'.join(retrieved_docs))\n",
        "\n",
        "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "      outputs = model.generate(**inputs, max_new_tokens=300)\n",
        "\n",
        "      response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "      return response"
      ],
      "metadata": {
        "id": "mtwEpSxi5Rjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4469d846-e4a7-4560-c882-0dcd0fc16945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clarification Rag (Self Contained)"
      ],
      "metadata": {
        "id": "8NZIYvzUwO8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_mode:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "  from rank_bm25 import BM25Okapi\n",
        "\n",
        "  # Step 1: Clarification Prompt Dataset\n",
        "  clarification_data = [\n",
        "      {\"query\": \"types?\", \"clarification\": \"Are you referring to types of learning, algorithms, or data?\"},\n",
        "      {\"query\": \"classification?\", \"clarification\": \"Do you mean classification algorithms in machine learning?\"},\n",
        "      {\"query\": \"supervised?\", \"clarification\": \"Are you asking about supervised learning methods?\"},\n",
        "      {\"query\": \"examples of models\", \"clarification\": \"Are you referring to machine learning models or statistical models?\"},\n",
        "      {\"query\": \"algorithms?\", \"clarification\": \"Could you specify what type of algorithms you mean?\"},\n",
        "      {\"query\": \"data?\", \"clarification\": \"Do you mean data types, sources, or structure?\"},\n",
        "      {\"query\": \"regression?\", \"clarification\": \"Are you referring to linear or logistic regression, or something else?\"},\n",
        "      {\"query\": \"clustering?\", \"clarification\": \"Do you want to know about clustering algorithms like K-Means or DBSCAN?\"},\n",
        "      {\"query\": \"accuracy?\", \"clarification\": \"Are you referring to model accuracy, evaluation, or metrics?\"},\n",
        "      {\"query\": \"features?\", \"clarification\": \"Do you mean input features in a dataset?\"},\n",
        "      {\"query\": \"deep learning?\", \"clarification\": \"Are you asking about deep learning models, or a specific framework?\"},\n",
        "      {\"query\": \"AI?\", \"clarification\": \"Do you mean artificial intelligence in general or a specific application?\"},\n",
        "      {\"query\": \"libraries?\", \"clarification\": \"Are you asking about Python libraries for data science or ML?\"},\n",
        "      {\"query\": \"tools?\", \"clarification\": \"Are you referring to development tools, ML tools, or data tools?\"},\n",
        "      {\"query\": \"Python?\", \"clarification\": \"Are you asking about Python basics or Python for data science?\"},\n",
        "      {\"query\": \"train model?\", \"clarification\": \"Are you asking how to train a machine learning model?\"},\n",
        "      {\"query\": \"unsupervised?\", \"clarification\": \"Do you mean unsupervised learning algorithms like clustering or dimensionality reduction?\"},\n",
        "      {\"query\": \"evaluation?\", \"clarification\": \"Are you asking about evaluation metrics like accuracy, precision, recall, etc.?\"},\n",
        "      {\"query\": \"visualize?\", \"clarification\": \"Do you want to know how to visualize data or model outputs?\"},\n",
        "      {\"query\": \"sklearn?\", \"clarification\": \"Are you asking about how to use the scikit-learn library?\"},\n",
        "      {\"query\": \"NLP?\", \"clarification\": \"Are you referring to Natural Language Processing tasks like sentiment analysis or translation?\"},\n",
        "      {\"query\": \"datasets?\", \"clarification\": \"Do you want suggestions for datasets or how to process them?\"},\n",
        "      {\"query\": \"hyperparameters?\", \"clarification\": \"Do you want to know how to tune model hyperparameters?\"},\n",
        "      {\"query\": \"EDA?\", \"clarification\": \"Are you asking about Exploratory Data Analysis techniques?\"},\n",
        "      {\"query\": \"metrics?\", \"clarification\": \"Are you referring to model performance metrics or business KPIs?\"}\n",
        "  ]\n",
        "\n",
        "  # Step 2: BM25 Index\n",
        "  clarification_queries = [item[\"query\"] for item in clarification_data]\n",
        "  tokenized_queries = [q.lower().split() for q in clarification_queries]\n",
        "  bm25 = BM25Okapi(tokenized_queries)\n",
        "\n",
        "  # Step 3: Clarification Lookup Function\n",
        "  def get_clarification(user_query, threshold=1.5):\n",
        "      query_tokens = user_query.lower().split()\n",
        "      scores = bm25.get_scores(query_tokens)\n",
        "      best_idx = scores.argmax()\n",
        "\n",
        "      if scores[best_idx] >= threshold:\n",
        "          return clarification_data[best_idx][\"clarification\"]\n",
        "      else:\n",
        "          return None  # No confident match\n",
        "\n",
        "  # Step 4: Main Clarifier Wrapper\n",
        "  def clarification_rag_pipeline(user_query, retrieved_docs=None):\n",
        "      clarification = get_clarification(user_query)\n",
        "\n",
        "      if clarification:\n",
        "          return f\"Just to clarify: {clarification}\"\n",
        "\n",
        "  # Else, proceed to main RAG pipeline\n",
        "      return run_main_rag_pipeline(user_query, retrieved_docs)\n"
      ],
      "metadata": {
        "id": "TDDoMLzlsTtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6f0a9a-d289-4aad-fdd8-46ca824c1ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip this cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsPoM2rokcJb"
      },
      "source": [
        "## Generator\n",
        "Language model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOpnc2tEgtIc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "bff7ed6c-f1d5-4ad9-a448-47a150b629b3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif improved_rag:\\n  print(\"Skip this cell\")\\nelse:\\n  print(\"Run this cell\")\\n\\n  model_name = \\'TinyLlama/TinyLlama-1.1B-Chat-v1.0\\'\\n  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\\n  model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)\\n\\n  inputs = tokenizer.encode(rag_prompts, return_tensors=\\'pt\\').to(device)    # rag_prompts / plain_prompts\\n  outputs = model.generate(inputs, max_length=1500, num_return_sequences=1, repetition_penalty=1.2)\\n  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n  print(textwrap.fill(answer[len(rag_prompts):], width=100))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "'''\n",
        "if improved_rag:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "\n",
        "  model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
        "\n",
        "  inputs = tokenizer.encode(rag_prompts, return_tensors='pt').to(device)    # rag_prompts / plain_prompts\n",
        "  outputs = model.generate(inputs, max_length=1500, num_return_sequences=1, repetition_penalty=1.2)\n",
        "  answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  print(textwrap.fill(answer[len(rag_prompts):], width=100))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language model: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct"
      ],
      "metadata": {
        "id": "ZYJBesYUePwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpHBnMjh_ez0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "cf6284d7f7a44e8ca9e680017f855a9c",
            "411499428c9347b88d453e47e1ff2ca0",
            "ed0b447441364d32916f9d64b614296f",
            "acde28d8d99346e785d1f1ade34a33a8",
            "6359528adc284cf6b3dbe15b339be830",
            "f812761779e34e02bec519440535ed08",
            "bd8cd31775e34546be23560582a0da31",
            "2946f3277b8944b89559cb82e414aed8",
            "c9786871a7424832b91ba7cae26fb43a",
            "f78c58deb5994a5a89a96b63d5860a76",
            "2d2a39c13a0e491aa405312974e280a7"
          ]
        },
        "outputId": "4743c419-e079-4a72-ae11-fedd9bfd782c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run this cell\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf6284d7f7a44e8ca9e680017f855a9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings in NLP refer to long vectors of real numbers that represent the knowledge associated with\n",
            "tokens, such as words or phrases. These vectors are learned during training and implicitly define a\n",
            "representation space that determines the meaning of the tokens. The meaning of two tokens with\n",
            "similar embeddings is similar, indicating that the embeddings capture semantic relationships between\n",
            "tokens. In the context of NLP, embeddings are calculated by self-attention computing correlations\n",
            "between linear projections of input embeddings, allowing models to capture subtle semantic\n",
            "properties of language. (Source: Knowledge Base)\n"
          ]
        }
      ],
      "source": [
        "# For original approach - without the fine-tuned model\n",
        "if improved_rag:\n",
        "  print(\"Skip this cell\")\n",
        "else:\n",
        "  print(\"Run this cell\")\n",
        "\n",
        "  # Configuring 4-bit Quantization Parameters\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type='nf4',\n",
        "      bnb_4bit_compute_dtype=torch.float16\n",
        "  )\n",
        "\n",
        "  # Loading the model and its tokenizer\n",
        "  model_id_1 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "  tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)\n",
        "  model_1 = AutoModelForCausalLM.from_pretrained(\n",
        "      model_id_1,\n",
        "      quantization_config=quantization_config,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  # Inference function\n",
        "  def generate_response(prompt, model, tokenizer, max_length=8000):\n",
        "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "      # Generate with the prompt tokens\n",
        "      prompt_length = inputs.input_ids.shape[1]\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_new_tokens=max_length,\n",
        "          pad_token_id=tokenizer.eos_token_id\n",
        "      )\n",
        "\n",
        "      # Only decode the newly generated tokens\n",
        "      response = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)\n",
        "      return response.strip()\n",
        "\n",
        "  # Generate an answer\n",
        "  answer = generate_response(rag_prompts, model_1, tokenizer_1)\n",
        "  print(textwrap.fill(answer, width=100))\n",
        "\n",
        "  # Release VRAM\n",
        "  del model_1\n",
        "  del tokenizer_1\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA adaptor: https://huggingface.co/Yanzhii/llama-3.2-3b-raft-adapter"
      ],
      "metadata": {
        "id": "iftpwov2eUqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if improved_rag:\n",
        "  print(\"Run this cell\")\n",
        "\n",
        "  # Configuration for model loading\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type='nf4',\n",
        "      bnb_4bit_compute_dtype=torch.float16\n",
        "  )\n",
        "\n",
        "  # Variables setup\n",
        "  base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "  hf_username = \"Yanzhii\"  # Hugging Face username\n",
        "  adapter_name = \"llama-3.2-3b-raft-adapter\"  # Adapter name\n",
        "  adapter_repo_id = f\"{hf_username}/{adapter_name}\"\n",
        "\n",
        "  print(f\"Loading base model: {base_model_id}\")\n",
        "\n",
        "  # Load tokenizer and base model\n",
        "  tokenizer_ft = AutoTokenizer.from_pretrained(base_model_id)\n",
        "  base_model = AutoModelForCausalLM.from_pretrained(\n",
        "      base_model_id,\n",
        "      quantization_config=quantization_config,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "\n",
        "  print(f\"Loading adapter: {adapter_repo_id}\")\n",
        "\n",
        "  # Load and merge with the LoRA adapter\n",
        "  model_ft = PeftModel.from_pretrained(base_model, adapter_repo_id)\n",
        "  model_ft.eval()  # Set to evaluation mode\n",
        "\n",
        "  # Generate response and separate CoT and answer parts\n",
        "  def generate_raft_response(messages, max_length=2000):\n",
        "      # Apply chat template to convert messages to model input format\n",
        "      inputs = tokenizer_ft.apply_chat_template(\n",
        "          messages,\n",
        "          add_generation_prompt=True,\n",
        "          return_tensors=\"pt\"\n",
        "      ).to(\"cuda\")\n",
        "\n",
        "      # Fix attention mask issue\n",
        "      attention_mask = torch.ones_like(inputs)\n",
        "\n",
        "      # Generate response\n",
        "      outputs = model_ft.generate(\n",
        "          input_ids=inputs,\n",
        "          attention_mask=attention_mask,\n",
        "          max_new_tokens=max_length,\n",
        "          pad_token_id=tokenizer_ft.eos_token_id,\n",
        "          temperature=0.7,\n",
        "          top_p=0.9\n",
        "      )\n",
        "\n",
        "      # Decode only the generated part\n",
        "      response_ids = outputs[0][inputs.shape[-1]:]\n",
        "      response_text = tokenizer_ft.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "      # Split response at <ANSWER> tag\n",
        "      parts = response_text.split(\"<ANSWER>\")\n",
        "\n",
        "      cot_part = parts[0].strip() if len(parts) > 0 else \"\"\n",
        "      answer_part = parts[1].strip() if len(parts) > 1 else response_text.strip()\n",
        "\n",
        "      # Remove leading colon and spaces from answer if present\n",
        "      if answer_part.startswith(\":\"):\n",
        "          answer_part = answer_part[1:].strip()\n",
        "\n",
        "      return cot_part, answer_part\n",
        "\n",
        "  # Generate answer with fine-tuned model\n",
        "  print(\"Generating response using RAFT fine-tuned model...\\n\")\n",
        "  raft_cot, raft_answer = generate_raft_response(messages)\n",
        "\n",
        "  print(\"Chain of Thought:\")\n",
        "  print(textwrap.fill(raft_cot, width=100))\n",
        "\n",
        "  print(\"\\nFinal Answer:\")\n",
        "  print(textwrap.fill(raft_answer, width=100))\n",
        "\n",
        "  # Release VRAM\n",
        "  del model_ft\n",
        "  del base_model\n",
        "  del tokenizer_ft\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "else:\n",
        "  print(\"Skip this cell\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "d623f559765e48e99f8f937767bc8be0",
            "fc9a8bd9ea354251b729109b8cc7890a",
            "172289d2007e4a8093e556754e2eb1d4",
            "496746e263f5482dbec2b40e40683728",
            "9900e62a59dd4cf2b0d19cf602cef718",
            "ff3b4c12d55f442cb33986771fb51169",
            "fc8eabffecc64e9e820115aa312ca6e3",
            "f124db3f80a6431a9964c4e790293df8",
            "f45e86a0805641c0b083e89fd295960d",
            "dc4dc37f5d7542e5a141b1ac7c742035",
            "330d281414ef4ca19fc9258d09761f9e"
          ]
        },
        "id": "m88aMCK-ODoa",
        "outputId": "d29771ac-f331-4ff6-dfe8-cd1a143a5fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run this cell\n",
            "Loading base model: meta-llama/Llama-3.2-3B-Instruct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d623f559765e48e99f8f937767bc8be0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading adapter: Yanzhii/llama-3.2-3b-raft-adapter\n",
            "Generating response using RAFT fine-tuned model...\n",
            "\n",
            "Chain of Thought:\n",
            "To answer the question about the meaning of embeddings in NLP, we need to carefully examine the\n",
            "context provided.  1. **Contextual Clues**: The context discusses the use of embeddings in Natural\n",
            "Language Processing (NLP). It mentions that embeddings are long vectors of real numbers and provide\n",
            "a way to represent the knowledge associated with the tokens.  2. **Key Information**: The context\n",
            "highlights the role of embeddings in determining the meaning of embeddings. It states that during\n",
            "training, a model implicitly defines a representation space that determines the meaning of\n",
            "embeddings. This indicates that embeddings are not just numerical representations but also carry\n",
            "semantic meaning.  3. **Semantic Interpretation**: The context suggests that embeddings are\n",
            "contextual, meaning they are influenced by the context in which they appear. For example, embeddings\n",
            "from the [CLS] token are different from those from other tokens in the same context.  4. **Embedding\n",
            "in NLP**: In the context of NLP, embeddings are used to capture the semantic meaning of words or\n",
            "tokens. They are often used as inputs to machine learning models to predict the meaning or intent\n",
            "behind a piece of text.  5. **Conclusion**: Based on the contextual clues and key information,\n",
            "embeddings in NLP are a way to represent the semantic meaning of tokens in a numerical format. They\n",
            "are used to capture the essence of the tokens and are influenced by the context in which they\n",
            "appear.\n",
            "\n",
            "Final Answer:\n",
            "Embeddings in NLP are a way to represent the semantic meaning of tokens in a numerical format,\n",
            "capturing the essence of the tokens and influenced by their context.\n",
            "\n",
            "Memory cleared\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "v3NynYyZIFJj",
        "I92A7m94NZo0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf6284d7f7a44e8ca9e680017f855a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_411499428c9347b88d453e47e1ff2ca0",
              "IPY_MODEL_ed0b447441364d32916f9d64b614296f",
              "IPY_MODEL_acde28d8d99346e785d1f1ade34a33a8"
            ],
            "layout": "IPY_MODEL_6359528adc284cf6b3dbe15b339be830"
          }
        },
        "411499428c9347b88d453e47e1ff2ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f812761779e34e02bec519440535ed08",
            "placeholder": "​",
            "style": "IPY_MODEL_bd8cd31775e34546be23560582a0da31",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ed0b447441364d32916f9d64b614296f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2946f3277b8944b89559cb82e414aed8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9786871a7424832b91ba7cae26fb43a",
            "value": 2
          }
        },
        "acde28d8d99346e785d1f1ade34a33a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78c58deb5994a5a89a96b63d5860a76",
            "placeholder": "​",
            "style": "IPY_MODEL_2d2a39c13a0e491aa405312974e280a7",
            "value": " 2/2 [00:16&lt;00:00,  7.52s/it]"
          }
        },
        "6359528adc284cf6b3dbe15b339be830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f812761779e34e02bec519440535ed08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8cd31775e34546be23560582a0da31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2946f3277b8944b89559cb82e414aed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9786871a7424832b91ba7cae26fb43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f78c58deb5994a5a89a96b63d5860a76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2a39c13a0e491aa405312974e280a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d623f559765e48e99f8f937767bc8be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc9a8bd9ea354251b729109b8cc7890a",
              "IPY_MODEL_172289d2007e4a8093e556754e2eb1d4",
              "IPY_MODEL_496746e263f5482dbec2b40e40683728"
            ],
            "layout": "IPY_MODEL_9900e62a59dd4cf2b0d19cf602cef718"
          }
        },
        "fc9a8bd9ea354251b729109b8cc7890a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3b4c12d55f442cb33986771fb51169",
            "placeholder": "​",
            "style": "IPY_MODEL_fc8eabffecc64e9e820115aa312ca6e3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "172289d2007e4a8093e556754e2eb1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f124db3f80a6431a9964c4e790293df8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f45e86a0805641c0b083e89fd295960d",
            "value": 2
          }
        },
        "496746e263f5482dbec2b40e40683728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc4dc37f5d7542e5a141b1ac7c742035",
            "placeholder": "​",
            "style": "IPY_MODEL_330d281414ef4ca19fc9258d09761f9e",
            "value": " 2/2 [00:13&lt;00:00,  6.30s/it]"
          }
        },
        "9900e62a59dd4cf2b0d19cf602cef718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3b4c12d55f442cb33986771fb51169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc8eabffecc64e9e820115aa312ca6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f124db3f80a6431a9964c4e790293df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f45e86a0805641c0b083e89fd295960d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc4dc37f5d7542e5a141b1ac7c742035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "330d281414ef4ca19fc9258d09761f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}